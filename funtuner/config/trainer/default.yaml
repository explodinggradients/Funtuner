_target_: transformers.TrainingArguments
output_dir: "."
learning_rate: 1e-3
gradient_checkpointing: true
gradient_accumulation_steps: 64
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1e-12
weight_decay: 0.001
eval_steps: 100
save_steps: 100
num_train_epochs: 1
logging_steps: 10
max_grad_norm: 1.0
save_total_limit: 4
fp16: true
lr_scheduler_type: cosine
warmup_ratio: 0.15
evaluation_strategy: steps
use_legacy_prediction_loop: false
