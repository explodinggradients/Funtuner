defaults:
  - trainer: default
model: openlm-research/open_llama_7b
log_dir: "/scratch/c.scmse/Funtuner-logs"
log_wandb: true
run_name: ""
wandb_entity: "shahules786"
max_length: 2048
per_digit_tokens: False
special_tokens:
  eos_token: "</s>"
  sep_token: "<sep>"
  pad_token: "<pad>"
datasets:

  - Dahoas/cot_gsm8k:
        split: ["train","val"]
  - psmathur/WizardLM_Orca:
        split: ["train"]

validation_size: 0.02
deepspeed: true
deepspeed_config: "./funtuner/config/zero2.json"
LoRa: true 
LoraConfig:
  r: 8
  target_modules: all
  lora_alpha: 16
  bias: none
  lora_dropout: 0.05
  task_type: CAUSAL_LM
  inference_mode: false
qlora: true
qlora_config:
  double_quant: true
  quant_type: nf4
load_in_4_bit: true
load_in_8_bit: false
template: alpaca-lora
