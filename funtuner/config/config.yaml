defaults:
  - trainer: default
model: EleutherAI/gpt-neo-125m
log_dir: "/scratch/c.scmse/Funtuner-logs"
log_wandb: true
run_name: ""
wandb_entity: "shahules786"
max_length: 512
per_digit_tokens: False
special_tokens:
  eos_token: "</s>"
  sep_token: "<sep>"
  pad_token: "<pad>"
datasets:

  - databricks/databricks-dolly-15k:
        split: ["train"]

validation_size: 0.1
deepspeed: true
deepspeed_config: "./funtuner/config/zero2.json"
LoRa: true 
LoraConfig:
  r: 8
  target_modules: ["query_key_value"]
  lora_alpha: 16
  bias: none
  lora_dropout: 0.05
  task_type: CAUSAL_LM
  inference_mode: false
eight_bit_training: false
template: alpaca-lora