defaults:
  - trainer: default
model: EleutherAI/gpt-neo-125m
log_dir: "Funtuner-logs"
log_wandb: true
run_name: ""
wandb_entity: "shahules786"
max_length: 512
per_digit_tokens: False
special_tokens:
  eos_token: "</s>"
  sep_token: "<sep>"
  pad_token: "<pad>"
datasets:

  - databricks/databricks-dolly-15k:
        split: ["train"]

validation_size: 0.1
deepspeed: false
deepspeed_config: "./funtuner/config/zero2.json"
LoRa: true 
LoraConfig:
  r: 8
  target_modules: ["q_proj", "v_proj"]
  lora_alpha: 16
  bias: none
  lora_dropout: 0.05
eight_bit_training: false
template: alpaca-lora